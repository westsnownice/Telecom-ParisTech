{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#version MAO Xinyu\n",
    "rfSize = 6\n",
    "numCentroids = 800\n",
    "numPatches = 200000\n",
    "CIFAR_DIM = [32,32,3]\n",
    "\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "import sys\n",
    "from six.moves import cPickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn import preprocessing\n",
    "import scipy as sp\n",
    "from scipy.sparse import coo_matrix\n",
    "from matplotlib import pyplot\n",
    "import math\n",
    "from sklearn.feature_extraction import image\n",
    "\n",
    "import time\n",
    "\n",
    "import progressbar\n",
    "\n",
    "def load_batch(fpath, label_key='labels'):\n",
    "\t\n",
    "\tf = open(fpath, 'rb')\n",
    "\tif sys.version_info < (3,): #si python 2 pas de soucis, sinon py3 decode\n",
    "\t\td = cPickle.load(f) #Pour désérialiser (ou charge, ou unpickle) un fichier de cornichon(a pickled file), utilisez cPickle.load:\n",
    "\telse:\n",
    "\t\td = cPickle.load(f, encoding=\"bytes\")\n",
    "\t\t# decode utf8\n",
    "\t\tfor k, v in d.items():\n",
    "\t\t\tdel(d[k])\n",
    "\t\t\td[k.decode(\"utf8\")] = v\n",
    "\tf.close()\n",
    "\tdata = d[\"data\"]\n",
    "\n",
    "\tlabels = d[label_key]\n",
    "\n",
    "\tdata = data.reshape(data.shape[0], 3072)\n",
    "\treturn data, labels\n",
    "\t\n",
    "def load_data():\n",
    "\t\"\"\"\n",
    "\tPermet de charger les donnees et les transformer en donnees de train et test\n",
    "\t\"\"\"\n",
    "\n",
    "\tdirname = \"./\"\n",
    "\n",
    "\tnb_train_samples = 50000\n",
    "\n",
    "\tX_train = np.zeros((nb_train_samples, 3072), dtype=\"uint8\") #Return a new array of given shape and type, filled with zeros.\n",
    "\n",
    "\ty_train = np.zeros((nb_train_samples,), dtype=\"uint8\")\n",
    "\n",
    "\t#open the file\n",
    "\tfor i in range(1, 6):\n",
    "\t\tfpath = os.path.join(dirname, 'data_batch_' + str(i))\n",
    "\t\tdata, labels = load_batch(fpath)\n",
    "\t\t\n",
    "\t\tX_train[(i-1)*10000:i*10000, :] = data #50 000 premieres = train\n",
    "\t\ty_train[(i-1)*10000:i*10000] = labels #50 000 premieres = train\n",
    "\n",
    "\tfpath = os.path.join(dirname, 'test_batch') \n",
    "\tX_test, y_test = load_batch(fpath)\n",
    "\n",
    "\ty_train = np.reshape(y_train, (len(y_train), 1))\n",
    "\n",
    "\t#on va chercher les donnees dans le test_batch\n",
    "\ty_test = np.reshape(y_test, (len(y_test), 1)) \n",
    "\n",
    "\treturn (X_train, y_train), (X_test, y_test)\n",
    "\t\n",
    "def kmeans(X=None, k=None, iterations=None):\n",
    "\t\n",
    "\tcentroids = np.random.randn(k, np.size(X, axis=1)) * 0.1 #Initialise les centroids au hasard\n",
    "\t\n",
    "\tBATCH_SIZE = 1000 #nombre d'éléments par iteration\n",
    "\t\n",
    "\tprogress = progressbar.ProgressBar(widgets=['K-means iterations',progressbar.Bar(), ' ', progressbar.Percentage(), ' ', progressbar.ETA()]) #进度条的显示\n",
    "\tfor itr in progress(range(1,iterations+1)):\n",
    "\t\ttime.sleep(0.001)\n",
    "\t\tprogress.update(itr)\n",
    "\n",
    "\t\tc2 = 0.5 * np.sum(centroids ** 2,axis=1, keepdims=True) # c2.shape = (K,1) somme des carres à minimiser\n",
    "\t\t\n",
    "\t\tsummation = np.zeros((k, np.size(X, axis=1))) #summation.shape = (K, ) Mean est non associatif, donc décompose le calcul en 2 parties associatives : la somme et le compteur\n",
    "\t\tcounts = np.zeros((k, 1))\n",
    "\t\t\n",
    "\t\ti = 0\n",
    "\t\twhile i < np.size(X, 0):\t\n",
    "\t\t\tlastIndex = min(i + BATCH_SIZE, np.size(X, 0))\n",
    "\t\t\tm = lastIndex - i\n",
    "\t\t\t\n",
    "\t\t\ta = np.dot(centroids,np.transpose(X[i:lastIndex, :])) #a.shape = (K, BATCH_SIZE)\n",
    "\t\t\tb = a - c2 #b.shape = (K, BATCH_SIZE) - (K,1)\n",
    "\n",
    "\t\t\tval = np.max(b,axis=0,keepdims=True) # On recupere la valeur maximale de chaque colonne\n",
    "\t\t\tlabels = np.argmax(b,axis=0) # On recupere la position de la valeur maximale\n",
    "\n",
    "\t\t\tS = np.zeros((BATCH_SIZE, k)) #on crée une matrice creuse (sparse matrix) avec bcp de zeros 1000 * K\n",
    "\t\t\tS[range(BATCH_SIZE), labels] = 1 #On met un 1 à chaque position trouvée (càd 1 si x ∈ argmin mu(z) − z_i, 0 sinon ), chaque donnée est assignée à la classe du centre le plus proche\n",
    "\t\t\t\n",
    "\t\t\tsummation = summation + np.dot(np.transpose(S),X[i:lastIndex, :]) \n",
    "\t\t\ttp = np.transpose(np.sum(S,0, keepdims=True))\n",
    "\t\t\tcounts = counts + tp\n",
    "\t\t\ti += BATCH_SIZE\n",
    "\t\t\t\n",
    "\t\twith np.errstate(invalid='ignore'):\n",
    "\t\t\tcentroids = summation / counts #on met à jour les moyennes\n",
    "\t\t\n",
    "\t\twhere_are_NaNs = np.isnan(centroids)\n",
    "\t\tcentroids[where_are_NaNs] = 0\n",
    "\t\t\n",
    "\t\tassert not np.any(np.isnan(centroids))\n",
    "\t\t\n",
    "\treturn centroids\n",
    "\n",
    "\t\n",
    "def show_centroids(centroids, rfSize, normalize=True):\n",
    "\t\"\"\"\n",
    "\tAffiche la grille des centroids\n",
    "\n",
    "\tOn calcule quelle taille x par x on veut, on reshape les centroids, on les place dans la grille puis on affiche le tout\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t\n",
    "\tchannels = centroids.shape[1] / (rfSize * rfSize) # = 3, RGB\n",
    "\n",
    "\tn_centroids = centroids.shape[0]\n",
    "\tcols = int(math.sqrt(n_centroids))\n",
    "\trows = math.ceil(n_centroids / cols)\n",
    "\n",
    "\timages = np.ones((rows * (rfSize + 1),cols * (rfSize + 1), channels ), dtype=\"uint8\")\n",
    "\tprint images.shape\n",
    "\t\n",
    "\tfor i in xrange(n_centroids):\n",
    "\t\tthis_image = centroids[i]\n",
    "\t\tthis_image = this_image.reshape((rfSize, rfSize, 3))\n",
    "\t\t\t\n",
    "\t\t_row = math.floor(i / cols)\n",
    "\t\t_row_end = (rfSize + 1) + 1 \n",
    "\t\t\t\n",
    "\t\t_col = i % cols\n",
    "\t\t_col_end = (rfSize + 1) + 1 \n",
    "\t\t\n",
    "\t\timages[(_row * _row_end) :((_row + 1) *_row_end)-1,(_col * _col_end):((_row + 1) *_col_end) - 1] = this_image\n",
    "\n",
    "\tif normalize:\n",
    "\t\t# Normalize it all\n",
    "\t\tmin = -1.5\n",
    "\t\tmax = 1.5\n",
    "\t\timages = (images - min) / (max - min)\n",
    "\tplt.imshow(images)\n",
    "\tplt.show()\n",
    "    \n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    On normalise chaque patch en soustrayant la moyenne et en divisant par la variance pour réduire le constraste\n",
    "    \"\"\"\n",
    "    temp1 = x - x.mean(1, keepdims=True)\n",
    "    temp2 = np.sqrt(x.var(1, keepdims=True) + 10)\n",
    "\n",
    "    return temp1 / temp2\n",
    "\n",
    "def getPatch(numPatches, Xtrain, patch_size):\n",
    "\t\n",
    "\tpatches = np.zeros((numPatches, rfSize * rfSize * 3), dtype=\"uint8\")  #shape 50000, 32 * 32 * 3\n",
    "\tXtrain = np.reshape(Xtrain,(Xtrain.shape[0],32,32,3)) #On reshape les données d'entrées pour extraire les patches\n",
    "\t\t\n",
    "\tprogress = progressbar.ProgressBar(widgets=['Extracting patches',progressbar.Bar(), ' ', progressbar.Percentage(), ' ', progressbar.ETA()])\n",
    "\tfor i in progress(range(numPatches)):\n",
    "\t\tprogress.update(i)\n",
    "\t\t\n",
    "\t\trow = np.random.randint(CIFAR_DIM[0] - rfSize + 1) # distribution uniforme \n",
    "\t\tcol = np.random.randint(CIFAR_DIM[1] - rfSize + 1) \n",
    "\t\t\n",
    "\t\timg = Xtrain[i %  Xtrain.shape[0]]+1 #on extrait numPatches/50000 = n patch par images\n",
    "\t\n",
    "\t\tpatch = img[row:row + rfSize, col:col + rfSize] # 6 * 6 * 3\n",
    "\n",
    "\t\tb = patch.flatten() #chaque ligne représente un patch\n",
    "\n",
    "\t\tpatches[i] = b\n",
    "\t\n",
    "\treturn np.vstack(patches) #on transforme chaque patch un nparray\n",
    "\n",
    "\t\n",
    "def extract_features(X, centroids, rfSize, CIFAR_DIM):\n",
    "\t\n",
    "\tn_centroids = centroids.shape[0]\n",
    "\tnewX = np.zeros((X.shape[0], n_centroids*4)); #narray à return à la fin avec les images reformées égale à 4K\n",
    "\tprogress = progressbar.ProgressBar(widgets=['Extracting features',progressbar.Bar(), ' ', progressbar.Percentage(), ' '])\n",
    "\tfor i in progress(range(X.shape[0])):\n",
    "\n",
    "\t\tprogress.update(i)\n",
    "\t\t\n",
    "\t\t#extraction convolutionelle en plusieurs étapes\n",
    "\t\t\n",
    "\t\t#on sépare les couleurs\n",
    "\t\tr = X[i,:1024]\n",
    "\t\tg = X[i,1024:2048]\n",
    "\t\tb = X[i,2048:]\n",
    "\t\t\n",
    "\t\trs = np.reshape(r, (32,32))\n",
    "\t\tgs = np.reshape(g, (32,32))\n",
    "\t\tbs = np.reshape(b, (32,32))\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#on reshape une image en colonne \n",
    "\t\trse = image.extract_patches_2d( rs, (rfSize,rfSize)) #729 * 6 * 6\n",
    "\t\tgse = image.extract_patches_2d( gs, (rfSize,rfSize))\n",
    "\t\tbse = image.extract_patches_2d( bs, (rfSize,rfSize))\n",
    "\t\t\n",
    "\t\trse = np.reshape(rse, ((len(rse), (rfSize * rfSize)))) # 729 * 36\n",
    "\t\tgse = np.reshape(gse, ((len(gse), (rfSize * rfSize))))\n",
    "\t\tbse = np.reshape(bse, ((len(bse), (rfSize * rfSize))))\n",
    "\t\t\n",
    "\t\tpatches = np.hstack((rse,gse,bse)) #chaque caractéristique est reformée en un grand vecteur de dimension 729 * 108 (729 blocs)\n",
    "\t\t\n",
    "\t\tpatches = normalize(patches)\n",
    "\t\t\n",
    "\t\txx = np.sum(patches ** 2, 1, keepdims=True) #729 vecteurs colonnes\n",
    "\t\tcc = np.sum(centroids ** 2, 1, keepdims=True).T #1600 vecteurs en colonne \n",
    "\t\txc = np.dot(patches, centroids.T) #matrice n_centroids * 729\n",
    "\t\t\n",
    "\t\tz = np.sqrt(cc + (xx - 2 * xc)) #distance = xx^2 + cc^2 - 2 * xx * cc\n",
    "\t\tmu = z.mean(1, keepdims=True) #distance moyenne entre chaque bloc et les n_centroids\n",
    "\t\tpatches = np.maximum(mu - z, 0) #on met 0 si la distance par rapport au centroid dépasse la moyenne\n",
    "\t\t\n",
    "\t\tprows = pcols = CIFAR_DIM[0] - rfSize + 1 # n - w + 1 = 27\n",
    "\t\tnum_centroids = centroids.shape[0]\n",
    "\t\t\n",
    "\t\tpatches = patches.reshape(prows, pcols, num_centroids) \n",
    "\t\t\n",
    "\t\thalfr = int(np.rint(prows / 2))\n",
    "\t\thalfc = int(np.rint(pcols / 2))\n",
    "\t\t\n",
    "\t\t#on récupère les 4 quadrants\n",
    "\t\tq1 = np.sum(patches[0:halfr, 0:halfc, :], (0, 1))\n",
    "\t\tq2 = np.sum(patches[halfr:, 0:halfc, :], (0, 1))\n",
    "\t\tq3 = np.sum(patches[0:halfr, halfc:, :], (0, 1))\n",
    "\t\tq4 = np.sum(patches[halfr:, halfc:, :], (0, 1))\n",
    "\t\t\n",
    "\t\t#on reforme les caractéristiques en concaténant les 4 quadrants\n",
    "\t\tnewX[i] = np.hstack((q1.flatten(), q2.flatten(), q3.flatten(), q4.flatten()))\n",
    "\t\t\n",
    "\t#return(n_images , 4 * K)\n",
    "\treturn np.vstack(newX)\n",
    "        \n",
    "\t\n",
    "if __name__ == '__main__':\n",
    "\t\n",
    "\t(X_train,y_train), (X_test, y_test) = load_data()\n",
    "\t\n",
    "\tprint \"Extraction des patches\"\n",
    "\t\n",
    "\tpatches = getPatch(numPatches, X_train,  rfSize)\n",
    "\t\n",
    "\ty_train = y_train.ravel()\n",
    "\ty_test = y_test.ravel()\n",
    "\n",
    "\tprint \"Normalisation\"\n",
    "\tpatches = normalize(patches)\n",
    "\n",
    "\tprint \"K-means\"\n",
    "\tcentroids = kmeans(patches, numCentroids, 10)\n",
    "\t\n",
    "\t\n",
    "\t#show_centroids(centroids, rfSize);\n",
    "\tprint \"Début extraction des features train\"\t\t\t\t\t \n",
    "\ttrainX = extract_features(X_train, centroids, rfSize, CIFAR_DIM)\n",
    "\t\n",
    "\tprint \"Début extraction des features test\"\n",
    "\ttestX = extract_features(X_test, centroids, rfSize, CIFAR_DIM)\n",
    "\t\n",
    "\ttrainX_mean = np.mean(trainX)\n",
    "\tprint \"Ecart type de trainX pour standardiser\"\n",
    "\ttrainX_st_dev = np.sqrt(np.var(trainX)+0.01)\n",
    "\t\n",
    "\tprint \"Soustraction de trainX par la moyenne pour standardiser\"\n",
    "\ttrainXSubMean = (trainX - trainX_mean) \n",
    "\tprint \"Soustraction de testX par la moyenne pour standardiser\"\t\t\n",
    "\ttestXSubMean = (testX - trainX_mean) \n",
    "\t\n",
    "\tprint \"Début Standardisation train\"\n",
    "\ttrainXs = trainXSubMean / trainX_st_dev\n",
    "\t\n",
    "\tprint \"Début Standardisation test\"\n",
    "\ttestXs = testXSubMean / trainX_st_dev\n",
    "\n",
    "\t\n",
    "\tfrom sklearn import svm\n",
    "\tprint \"Début Classification\"\n",
    "\tclf = svm.SVC(verbose=1, shrinking=False)\n",
    "\tclf.fit(trainXs, y_train) \n",
    "\t\n",
    "\n",
    "\tprint \"Début test\"\n",
    "\tres = clf.score(testXs, y_test)\n",
    "\t\n",
    "\tprint \"Accuracy test : \", res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
